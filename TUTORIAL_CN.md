# FlyMy.AI LoRA Trainer ä»£ç ä»“åº“è¯¦è§£

## ğŸ¯ å†™ç»™åˆå­¦è€…çš„è¯

ä½ å¥½ï¼å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡çœ‹è¿™ä¸ªé¡¹ç›®ï¼Œè§‰å¾—å¾ˆå¤æ‚ï¼Œå®Œå…¨ä¸ç”¨æ‹…å¿ƒã€‚è¿™ä»½æ•™ç¨‹ä¼š**ä¸€æ­¥ä¸€æ­¥**å¸¦ä½ ç†è§£è¿™ä¸ªä»£ç ä»“åº“çš„**æ¯ä¸€ä¸ªéƒ¨åˆ†**ã€‚

æˆ‘ä¼šç”¨æœ€ç®€å•çš„è¯­è¨€ï¼Œä»é›¶å¼€å§‹è®²è§£ã€‚ç›¸ä¿¡æˆ‘ï¼Œè¯»å®Œè¿™ä»½æ•™ç¨‹åï¼Œä½ ä¼šå¯¹æ•´ä¸ªé¡¹ç›®æœ‰æ¸…æ™°çš„è®¤è¯†ï¼

---

## ğŸ“š ç›®å½•

1. [è¿™ä¸ªé¡¹ç›®æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ](#1-è¿™ä¸ªé¡¹ç›®æ˜¯åšä»€ä¹ˆçš„)
2. [æ ¸å¿ƒæ¦‚å¿µè®²è§£](#2-æ ¸å¿ƒæ¦‚å¿µè®²è§£)
3. [ç›®å½•ç»“æ„è¯¦è§£](#3-ç›®å½•ç»“æ„è¯¦è§£)
4. [æ ¸å¿ƒä»£ç æ–‡ä»¶è®²è§£](#4-æ ¸å¿ƒä»£ç æ–‡ä»¶è®²è§£)
5. [æ•°æ®å‡†å¤‡è¯¦è§£](#5-æ•°æ®å‡†å¤‡è¯¦è§£)
6. [è®­ç»ƒæµç¨‹è¯¦è§£](#6-è®­ç»ƒæµç¨‹è¯¦è§£)
7. [æ¨ç†ä½¿ç”¨è¯¦è§£](#7-æ¨ç†ä½¿ç”¨è¯¦è§£)
8. [å®æˆ˜ç¤ºä¾‹](#8-å®æˆ˜ç¤ºä¾‹)
9. [å¸¸è§é—®é¢˜](#9-å¸¸è§é—®é¢˜)

---

## 1. è¿™ä¸ªé¡¹ç›®æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ

### 1.1 ç®€å•å›ç­”

**è¿™ä¸ªé¡¹ç›®æ˜¯ä¸€ä¸ª AI å›¾åƒç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒå·¥å…·**ã€‚

ç”¨å¤§ç™½è¯è¯´ï¼š
- ä½ æœ‰ä¸€äº›å›¾ç‰‡å’Œå®ƒä»¬çš„æè¿°æ–‡å­—
- ä½ æƒ³è®­ç»ƒä¸€ä¸ª AI æ¨¡å‹ï¼Œè®©å®ƒèƒ½å¤Ÿæ ¹æ®ä½ çš„é£æ ¼ç”Ÿæˆæ–°å›¾ç‰‡
- è¿™ä¸ªé¡¹ç›®å°±æ˜¯å¸®ä½ å®Œæˆè¿™ä¸ªè®­ç»ƒè¿‡ç¨‹çš„å·¥å…·

### 1.2 æ”¯æŒçš„æ¨¡å‹

è¿™ä¸ªé¡¹ç›®æ”¯æŒè®­ç»ƒä¸‰ç§ä¸»æµçš„ AI å›¾åƒç”Ÿæˆæ¨¡å‹ï¼š

1. **Qwen-Image**ï¼šé˜¿é‡Œå·´å·´å¼€å‘çš„ä¸­æ–‡å‹å¥½å›¾åƒç”Ÿæˆæ¨¡å‹
2. **Qwen-Image-Edit**ï¼šå¯ä»¥ç¼–è¾‘å›¾ç‰‡çš„æ¨¡å‹ï¼ˆæ¯”å¦‚æ”¹å˜å›¾ç‰‡ä¸­çš„å†…å®¹ï¼‰
3. **FLUX.1-dev**ï¼šä¸€ä¸ªéå¸¸å¼ºå¤§çš„äººåƒå’Œè§’è‰²ç”Ÿæˆæ¨¡å‹

### 1.3 ä»€ä¹ˆæ˜¯ LoRAï¼Ÿ

**LoRA**ï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§**é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒæŠ€æœ¯**ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦ LoRAï¼Ÿ**

æƒ³è±¡ä¸€ä¸‹ï¼š
- ä¸€ä¸ªå®Œæ•´çš„ AI å›¾åƒæ¨¡å‹å¯èƒ½æœ‰**å‡ åäº¿ä¸ªå‚æ•°**ï¼ˆå°±åƒå¤§è„‘ä¸­çš„ç¥ç»å…ƒï¼‰
- å¦‚æœä½ æƒ³è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œéœ€è¦ï¼š
  - è¶…çº§å¼ºå¤§çš„ GPUï¼ˆæ˜¾å¡ï¼‰
  - å¤§é‡çš„æ—¶é—´
  - å¾ˆå¤šè®­ç»ƒæ•°æ®

**LoRA çš„èªæ˜ä¹‹å¤„ï¼š**
- ä¸è®­ç»ƒæ•´ä¸ªæ¨¡å‹çš„æ‰€æœ‰å‚æ•°
- åªè®­ç»ƒä¸€å°éƒ¨åˆ†**é¢å¤–æ·»åŠ çš„å‚æ•°**ï¼ˆå¯èƒ½åªæœ‰å‡ ç™¾ä¸‡ä¸ªï¼‰
- å°±åƒç»™æ¨¡å‹åŠ äº†ä¸€ä¸ª"å°æ’ä»¶"ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹
- è®­ç»ƒé€Ÿåº¦å¿«ï¼Œéœ€è¦çš„æ˜¾å­˜å°ï¼Œæ•ˆæœè¿˜å¾ˆå¥½ï¼

**ä¸¾ä¸ªä¾‹å­ï¼š**
```
å®Œæ•´æ¨¡å‹è®­ç»ƒï¼šéœ€è¦ 80GB æ˜¾å­˜ï¼Œè®­ç»ƒ 10 å¤©
LoRA è®­ç»ƒï¼šéœ€è¦ 24GB æ˜¾å­˜ï¼Œè®­ç»ƒ 2-4 å°æ—¶
```

---

## 2. æ ¸å¿ƒæ¦‚å¿µè®²è§£

åœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š

### 2.1 ä»€ä¹ˆæ˜¯æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰ï¼Ÿ

**æ‰©æ•£æ¨¡å‹**æ˜¯ç›®å‰æœ€å…ˆè¿›çš„å›¾åƒç”ŸæˆæŠ€æœ¯ã€‚

**å·¥ä½œåŸç†ï¼ˆç”¨ç®€å•çš„æ¯”å–»ï¼‰ï¼š**

æƒ³è±¡ä½ åœ¨ç”»ç”»ï¼š
1. **æ­£å‘è¿‡ç¨‹**ï¼šæŠŠä¸€å¼ æ¸…æ™°çš„å›¾ç‰‡é€æ¸åŠ å…¥å™ªç‚¹ï¼Œæœ€åå˜æˆå®Œå…¨çš„å™ªå£°
2. **åå‘è¿‡ç¨‹ï¼ˆç”Ÿæˆï¼‰**ï¼šä»å™ªå£°å¼€å§‹ï¼ŒAI æ¨¡å‹é€æ­¥å»é™¤å™ªå£°ï¼Œæœ€åç”Ÿæˆæ¸…æ™°çš„å›¾ç‰‡

```
æ¸…æ™°å›¾ç‰‡ â†’ åŠ å™ª â†’ åŠ å™ª â†’ åŠ å™ª â†’ çº¯å™ªå£°
çº¯å™ªå£° â†’ å»å™ª â†’ å»å™ª â†’ å»å™ª â†’ æ¸…æ™°å›¾ç‰‡ âœ¨
```

### 2.2 ä»€ä¹ˆæ˜¯ Transformerï¼Ÿ

**Transformer** æ˜¯ AI æ¨¡å‹çš„ä¸€ç§æ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿ç†è§£å’Œç”Ÿæˆåºåˆ—æ•°æ®ã€‚

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼š
- Transformer è´Ÿè´£ç†è§£**æ–‡æœ¬æç¤ºè¯**ï¼ˆpromptï¼‰
- å®ƒæŠŠæ–‡å­—è½¬æ¢æˆ AI èƒ½ç†è§£çš„æ•°å­—è¡¨ç¤º
- ç„¶åæŒ‡å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹

### 2.3 ä»€ä¹ˆæ˜¯ VAEï¼Ÿ

**VAE**ï¼ˆVariational Autoencoderï¼Œå˜åˆ†è‡ªç¼–ç å™¨ï¼‰æ˜¯ä¸€ä¸ª**å›¾åƒå‹ç¼©å’Œè§£å‹å·¥å…·**ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦ VAEï¼Ÿ**

- ä¸€å¼  1024Ã—1024 çš„ RGB å›¾ç‰‡æœ‰ **300 å¤šä¸‡ä¸ªåƒç´ ç‚¹**
- ç›´æ¥å¤„ç†è¿™ä¹ˆå¤§çš„æ•°æ®ä¼šéå¸¸æ…¢
- VAE å¯ä»¥æŠŠå›¾ç‰‡å‹ç¼©æˆå°å¾—å¤šçš„"æ½œåœ¨è¡¨ç¤º"ï¼ˆlatentï¼‰

**å·¥ä½œæµç¨‹ï¼š**
```
åŸå§‹å›¾ç‰‡ (1024Ã—1024Ã—3) 
  â†“ VAE ç¼–ç 
æ½œåœ¨è¡¨ç¤º (å°å¾—å¤šï¼Œæ¯”å¦‚ 128Ã—128Ã—4)
  â†“ AI æ¨¡å‹åœ¨è¿™ä¸ªå°ç©ºé—´é‡Œå·¥ä½œ
æ½œåœ¨è¡¨ç¤º (å¤„ç†å)
  â†“ VAE è§£ç 
ç”Ÿæˆçš„å›¾ç‰‡ (1024Ã—1024Ã—3)
```

### 2.4 è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µ

#### 2.4.1 æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰

**æŸå¤±å‡½æ•°**è¡¡é‡ AI æ¨¡å‹çš„é¢„æµ‹æœ‰å¤š"é”™"ã€‚

- æŸå¤±å€¼è¶Šå°ï¼Œæ¨¡å‹é¢„æµ‹è¶Šå‡†ç¡®
- è®­ç»ƒçš„ç›®æ ‡å°±æ˜¯ä¸æ–­**é™ä½æŸå¤±å€¼**

#### 2.4.2 å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰

**å­¦ä¹ ç‡**æ§åˆ¶æ¨¡å‹æ¯æ¬¡æ›´æ–°å‚æ•°çš„æ­¥é•¿ã€‚

```
å­¦ä¹ ç‡å¤ªå¤§ï¼šæ¨¡å‹å¯èƒ½å­¦ä¸å¥½ï¼Œè·³æ¥è·³å»
å­¦ä¹ ç‡å¤ªå°ï¼šæ¨¡å‹å­¦å¾—å¤ªæ…¢
åˆé€‚çš„å­¦ä¹ ç‡ï¼šç¨³æ­¥è¿›æ­¥ âœ¨
```

#### 2.4.3 æ‰¹æ¬¡å¤§å°ï¼ˆBatch Sizeï¼‰

**æ‰¹æ¬¡å¤§å°**æ˜¯æ¯æ¬¡è®­ç»ƒä½¿ç”¨å¤šå°‘å¼ å›¾ç‰‡ã€‚

```
æ‰¹æ¬¡å¤§å° = 1ï¼šæ¯æ¬¡çœ‹ 1 å¼ å›¾
æ‰¹æ¬¡å¤§å° = 4ï¼šæ¯æ¬¡çœ‹ 4 å¼ å›¾ï¼Œç„¶åä¸€èµ·æ›´æ–°æ¨¡å‹
```

- æ‰¹æ¬¡è¶Šå¤§ï¼Œè®­ç»ƒè¶Šç¨³å®šï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜
- æ‰¹æ¬¡è¶Šå°ï¼Œæ˜¾å­˜å ç”¨å°‘ï¼Œä½†è®­ç»ƒå¯èƒ½ä¸å¤ªç¨³å®š

---

## 3. ç›®å½•ç»“æ„è¯¦è§£

è®©æˆ‘ä»¬çœ‹çœ‹é¡¹ç›®çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹éƒ½æ˜¯å¹²ä»€ä¹ˆçš„ï¼š

```
flymyai-lora-trainer/
â”œâ”€â”€ README.md                    # é¡¹ç›®è¯´æ˜æ–‡æ¡£ï¼ˆè‹±æ–‡ï¼‰
â”œâ”€â”€ TUTORIAL_CN.md              # æœ¬æ•™ç¨‹æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # Python ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ LICENSE                      # å¼€æºåè®®
â”‚
â”œâ”€â”€ assets/                      # èµ„æºæ–‡ä»¶å¤¹
â”‚   â”œâ”€â”€ flymy_transparent.png   # é¡¹ç›® Logo
â”‚   â”œâ”€â”€ lora.png                # ç¤ºä¾‹è¾“å‡ºå›¾ç‰‡
â”‚   â””â”€â”€ ...                     # å…¶ä»–ç¤ºä¾‹å›¾ç‰‡
â”‚
â”œâ”€â”€ image_datasets/             # æ•°æ®é›†å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ dataset.py              # æ ¸å¿ƒï¼šæ•°æ®åŠ è½½å™¨
â”‚   â””â”€â”€ control_dataset.py      # å›¾åƒç¼–è¾‘ä¸“ç”¨æ•°æ®åŠ è½½å™¨
â”‚
â”œâ”€â”€ train_configs/              # è®­ç»ƒé…ç½®æ–‡ä»¶å¤¹
â”‚   â”œâ”€â”€ train_lora.yaml         # Qwen-Image LoRA è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ train_lora_4090.yaml    # 24GB æ˜¾å­˜ä¼˜åŒ–é…ç½®
â”‚   â”œâ”€â”€ train_flux_config.yaml  # FLUX æ¨¡å‹è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ train_lora_qwen_edit.yaml  # å›¾åƒç¼–è¾‘è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ ...                     # å…¶ä»–é…ç½®æ–‡ä»¶
â”‚
â”œâ”€â”€ train.py                    # Qwen-Image LoRA è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ train_4090.py               # ä½æ˜¾å­˜ç‰ˆè®­ç»ƒç¨‹åº
â”œâ”€â”€ train_flux_lora.py          # FLUX LoRA è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ train_qwen_edit_lora.py     # å›¾åƒç¼–è¾‘è®­ç»ƒç¨‹åº
â”œâ”€â”€ train_full_qwen_image.py    # Qwen å®Œæ•´æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ train_kandinsky_lora.py     # Kandinsky æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ train_z_image_lora.py       # Z-Image æ¨¡å‹è®­ç»ƒ
â”‚
â”œâ”€â”€ inference.py                # æ¨ç†ï¼ˆç”Ÿæˆå›¾ç‰‡ï¼‰ç¨‹åº
â”œâ”€â”€ qwen_full_inference_example.py  # å®Œæ•´æ¨¡å‹æ¨ç†ç¤ºä¾‹
â”œâ”€â”€ qwen_image_lora_example.json    # ComfyUI å·¥ä½œæµæ–‡ä»¶
â”‚
â””â”€â”€ utils/                      # å·¥å…·å‡½æ•°
    â””â”€â”€ validate_dataset.py     # æ•°æ®é›†éªŒè¯å·¥å…·
```

### 3.1 é‡ç‚¹æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶å | ä½œç”¨ | é‡è¦ç¨‹åº¦ |
|--------|------|----------|
| `train.py` | Qwen-Image è®­ç»ƒçš„æ ¸å¿ƒä»£ç  | â­â­â­â­â­ |
| `train_flux_lora.py` | FLUX è®­ç»ƒçš„æ ¸å¿ƒä»£ç  | â­â­â­â­â­ |
| `image_datasets/dataset.py` | æ•°æ®åŠ è½½å’Œé¢„å¤„ç† | â­â­â­â­â­ |
| `train_configs/*.yaml` | è®­ç»ƒå‚æ•°é…ç½® | â­â­â­â­ |
| `inference.py` | ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆå›¾ç‰‡ | â­â­â­â­ |
| `requirements.txt` | å®‰è£…æ‰€éœ€çš„ Python åŒ… | â­â­â­ |

---

## 4. æ ¸å¿ƒä»£ç æ–‡ä»¶è®²è§£

ç°åœ¨æˆ‘ä»¬æ·±å…¥åˆ°ä»£ç å±‚é¢ï¼Œä¸€æ­¥æ­¥ç†è§£æ¯ä¸ªæ–‡ä»¶åœ¨åšä»€ä¹ˆã€‚

### 4.1 train.py - Qwen-Image è®­ç»ƒä¸»ç¨‹åº

è¿™æ˜¯ Qwen-Image LoRA è®­ç»ƒçš„æ ¸å¿ƒæ–‡ä»¶ã€‚è®©æˆ‘ä»¬åˆ†æ®µç†è§£ï¼š

#### 4.1.1 å¯¼å…¥å¿…è¦çš„åº“

```python
import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import torch     # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from accelerate import Accelerator  # åŠ é€Ÿè®­ç»ƒï¼ˆæ”¯æŒå¤š GPU ç­‰ï¼‰
from diffusers import QwenImagePipeline  # Qwen å›¾åƒç”Ÿæˆç®¡é“
from peft import LoraConfig  # LoRA é…ç½®
```

**è§£é‡Šï¼š**
- æ¯ä¸ª `import` éƒ½æ˜¯å¼•å…¥ä¸€ä¸ªå·¥å…·åº“
- å°±åƒåœ¨åšèœå‰å‡†å¤‡é£Ÿæå’Œå·¥å…·

#### 4.1.2 åŠ è½½é…ç½®

```python
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True)
    return args.config

args = OmegaConf.load(parse_args())
```

**è¿™æ®µä»£ç åšä»€ä¹ˆï¼Ÿ**
1. ä»å‘½ä»¤è¡Œè¯»å–é…ç½®æ–‡ä»¶è·¯å¾„
2. åŠ è½½ YAML é…ç½®æ–‡ä»¶
3. æŠŠé…ç½®å­˜å‚¨åœ¨ `args` å˜é‡ä¸­

**ä¸¾ä¾‹ï¼š**
```bash
python train.py --config ./train_configs/train_lora.yaml
```

#### 4.1.3 åˆå§‹åŒ– Accelerator

```python
accelerator = Accelerator(
    gradient_accumulation_steps=args.gradient_accumulation_steps,
    mixed_precision=args.mixed_precision,
)
```

**Accelerator æ˜¯ä»€ä¹ˆï¼Ÿ**
- è¿™æ˜¯ Hugging Face æä¾›çš„è®­ç»ƒåŠ é€Ÿå·¥å…·
- è‡ªåŠ¨å¤„ç†å¤š GPUã€æ··åˆç²¾åº¦è®­ç»ƒç­‰å¤æ‚æ“ä½œ
- è®©ä½ çš„ä»£ç å¯ä»¥è½»æ¾ä»å• GPU æ‰©å±•åˆ°å¤š GPU

**æ··åˆç²¾åº¦ï¼ˆMixed Precisionï¼‰ï¼š**
- æ­£å¸¸æƒ…å†µä¸‹ï¼Œæ•°å­—ç”¨ 32 ä½å­˜å‚¨ï¼ˆfloat32ï¼‰
- æ··åˆç²¾åº¦ç”¨ 16 ä½å­˜å‚¨ï¼ˆfloat16 æˆ– bfloat16ï¼‰
- **ä¼˜ç‚¹**ï¼šé€Ÿåº¦æ›´å¿«ï¼Œæ˜¾å­˜å ç”¨å‡åŠ
- **é£é™©**ï¼šç²¾åº¦ç¨å¾®é™ä½ï¼ˆä½†é€šå¸¸å½±å“å¾ˆå°ï¼‰

#### 4.1.4 åŠ è½½æ¨¡å‹ç»„ä»¶

```python
# 1. åŠ è½½æ–‡æœ¬ç¼–ç ç®¡é“
text_encoding_pipeline = QwenImagePipeline.from_pretrained(
    args.pretrained_model_name_or_path, 
    transformer=None,  # ä¸åŠ è½½ transformer
    vae=None,          # ä¸åŠ è½½ VAE
)

# 2. åŠ è½½ VAEï¼ˆå›¾åƒç¼–ç å™¨/è§£ç å™¨ï¼‰
vae = AutoencoderKLQwenImage.from_pretrained(
    args.pretrained_model_name_or_path,
    subfolder="vae",
)

# 3. åŠ è½½ Transformerï¼ˆæ ¸å¿ƒç”Ÿæˆæ¨¡å‹ï¼‰
flux_transformer = QwenImageTransformer2DModel.from_pretrained(
    args.pretrained_model_name_or_path,
    subfolder="transformer",
)
```

**ä¸ºä»€ä¹ˆåˆ†å¼€åŠ è½½ï¼Ÿ**
- æ¯ä¸ªç»„ä»¶æœ‰ä¸åŒçš„ä½œç”¨
- æˆ‘ä»¬åªæƒ³è®­ç»ƒ Transformer çš„ LoRA éƒ¨åˆ†
- VAE å’Œæ–‡æœ¬ç¼–ç å™¨ä¿æŒä¸å˜ï¼ˆå†»ç»“å‚æ•°ï¼‰

#### 4.1.5 é…ç½® LoRA

```python
lora_config = LoraConfig(
    r=args.rank,                              # LoRA ç§©ï¼ˆé€šå¸¸ 4-128ï¼‰
    lora_alpha=args.rank,                     # LoRA ç¼©æ”¾å› å­
    init_lora_weights="gaussian",             # åˆå§‹åŒ–æ–¹æ³•
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # è¦è®­ç»ƒçš„æ¨¡å—
)

flux_transformer.add_adapter(lora_config)
```

**å‚æ•°è§£é‡Šï¼š**

- **rank (r)**ï¼šLoRA çš„ç§©ï¼Œæ§åˆ¶ LoRA å±‚çš„å¤§å°
  - è¶Šå¤§ï¼šè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†å‚æ•°è¶Šå¤š
  - è¶Šå°ï¼šå‚æ•°å°‘ï¼Œè®­ç»ƒå¿«ï¼Œä½†å¯èƒ½å­¦ä¸å¥½
  - æ¨èå€¼ï¼š8-32

- **target_modules**ï¼šåœ¨å“ªäº›å±‚æ·»åŠ  LoRA
  - `to_q`ã€`to_k`ã€`to_v`ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„æŸ¥è¯¢ã€é”®ã€å€¼å±‚
  - `to_out.0`ï¼šè¾“å‡ºå±‚

#### 4.1.6 å†»ç»“ä¸è®­ç»ƒçš„å‚æ•°

```python
vae.requires_grad_(False)          # VAE ä¸è®­ç»ƒ
flux_transformer.requires_grad_(False)  # å…ˆå…¨éƒ¨å†»ç»“

# åªè§£å†» LoRA å‚æ•°
for n, param in flux_transformer.named_parameters():
    if 'lora' not in n:
        param.requires_grad = False  # ä¸æ˜¯ LoRAï¼Œå†»ç»“
    else:
        param.requires_grad = True   # æ˜¯ LoRAï¼Œè®­ç»ƒ
```

**requires_grad æ˜¯ä»€ä¹ˆï¼Ÿ**
- `True`ï¼šè¿™ä¸ªå‚æ•°ä¼šåœ¨è®­ç»ƒä¸­æ›´æ–°
- `False`ï¼šè¿™ä¸ªå‚æ•°å›ºå®šä¸å˜

#### 4.1.7 å‡†å¤‡ä¼˜åŒ–å™¨

```python
optimizer = torch.optim.AdamW(
    lora_layers,                    # åªä¼˜åŒ– LoRA å‚æ•°
    lr=args.learning_rate,          # å­¦ä¹ ç‡
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
)
```

**ä¼˜åŒ–å™¨æ˜¯ä»€ä¹ˆï¼Ÿ**
- ä¼˜åŒ–å™¨å†³å®šå¦‚ä½•æ›´æ–°æ¨¡å‹å‚æ•°
- **AdamW** æ˜¯ç›®å‰æœ€æµè¡Œçš„ä¼˜åŒ–å™¨ä¹‹ä¸€
- å®ƒä¼šæ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œæ™ºèƒ½è°ƒæ•´æ¯ä¸ªå‚æ•°

#### 4.1.8 è®­ç»ƒå¾ªç¯

```python
for epoch in range(1):
    for step, batch in enumerate(train_dataloader):
        img, prompts = batch  # è·å–ä¸€æ‰¹å›¾ç‰‡å’Œæ–‡æœ¬
        
        # 1. å°†å›¾ç‰‡ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤º
        pixel_latents = vae.encode(pixel_values).latent_dist.sample()
        
        # 2. æ·»åŠ å™ªå£°
        noise = torch.randn_like(pixel_latents)
        noisy_model_input = (1.0 - sigmas) * pixel_latents + sigmas * noise
        
        # 3. ç¼–ç æ–‡æœ¬æç¤ºè¯
        prompt_embeds = text_encoding_pipeline.encode_prompt(prompts)
        
        # 4. æ¨¡å‹é¢„æµ‹
        model_pred = flux_transformer(
            hidden_states=packed_noisy_model_input,
            encoder_hidden_states=prompt_embeds,
            timestep=timesteps,
        )
        
        # 5. è®¡ç®—æŸå¤±
        target = noise - pixel_latents
        loss = torch.mean((model_pred - target) ** 2)
        
        # 6. åå‘ä¼ æ’­ï¼Œæ›´æ–°å‚æ•°
        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()
```

**è®­ç»ƒæ­¥éª¤è¯¦è§£ï¼š**

1. **ç¼–ç å›¾ç‰‡**ï¼šæŠŠå›¾ç‰‡è½¬æ¢æˆæ½œåœ¨è¡¨ç¤ºï¼ˆå‹ç¼©ï¼‰
2. **æ·»åŠ å™ªå£°**ï¼šæ¨¡æ‹Ÿæ‰©æ•£è¿‡ç¨‹
3. **ç¼–ç æ–‡æœ¬**ï¼šæŠŠæç¤ºè¯è½¬æ¢æˆåµŒå…¥å‘é‡
4. **æ¨¡å‹é¢„æµ‹**ï¼šè®©æ¨¡å‹é¢„æµ‹å¦‚ä½•å»å™ª
5. **è®¡ç®—æŸå¤±**ï¼šæ¯”è¾ƒé¢„æµ‹å’ŒçœŸå®ç›®æ ‡
6. **æ›´æ–°å‚æ•°**ï¼šæ ¹æ®æŸå¤±è°ƒæ•´ LoRA å‚æ•°

### 4.2 image_datasets/dataset.py - æ•°æ®åŠ è½½å™¨

è¿™ä¸ªæ–‡ä»¶è´Ÿè´£åŠ è½½å’Œé¢„å¤„ç†è®­ç»ƒæ•°æ®ã€‚

#### 4.2.1 è‡ªå®šä¹‰æ•°æ®é›†ç±»

```python
class CustomImageDataset(Dataset):
    def __init__(self, img_dir, img_size=512, caption_type='txt'):
        # æ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡
        self.images = [
            os.path.join(img_dir, i) 
            for i in os.listdir(img_dir) 
            if '.jpg' in i or '.png' in i
        ]
        self.img_size = img_size
        self.caption_type = caption_type
```

**è¿™ä¸ªç±»åšä»€ä¹ˆï¼Ÿ**
- æ‰«æå›¾ç‰‡æ–‡ä»¶å¤¹ï¼Œæ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡
- è®°å½•å›¾ç‰‡è·¯å¾„å’Œå¤§å°
- å‡†å¤‡å¥½æ•°æ®åŠ è½½çš„åŸºç¡€è®¾æ–½

#### 4.2.2 åŠ è½½å•ä¸ªæ ·æœ¬

```python
def __getitem__(self, idx):
    # 1. éšæœºé€‰æ‹©ä¸€å¼ å›¾ç‰‡
    idx = random.randint(0, len(self.images) - 1)
    
    # 2. æ‰“å¼€å›¾ç‰‡
    img = Image.open(self.images[idx]).convert('RGB')
    
    # 3. è°ƒæ•´å¤§å°
    img = image_resize(img, self.img_size)
    
    # 4. è½¬æ¢ä¸ºå¼ é‡
    img = torch.from_numpy((np.array(img) / 127.5) - 1)
    img = img.permute(2, 0, 1)  # è°ƒæ•´ç»´åº¦é¡ºåº
    
    # 5. åŠ è½½å¯¹åº”çš„æ–‡æœ¬æè¿°
    txt_path = self.images[idx].rsplit('.', 1)[0] + '.txt'
    prompt = open(txt_path, encoding='utf-8').read()
    
    return img, prompt
```

**æ•°æ®é¢„å¤„ç†æ­¥éª¤ï¼š**

1. **æ‰“å¼€å›¾ç‰‡**ï¼šä½¿ç”¨ PIL åº“è¯»å–
2. **è°ƒæ•´å¤§å°**ï¼šç»Ÿä¸€å›¾ç‰‡å°ºå¯¸
3. **å½’ä¸€åŒ–**ï¼šæŠŠåƒç´ å€¼ä» [0, 255] è½¬æ¢åˆ° [-1, 1]
4. **è°ƒæ•´ç»´åº¦**ï¼šä» (H, W, C) è½¬æ¢åˆ° (C, H, W)
5. **åŠ è½½æ–‡æœ¬**ï¼šè¯»å–å¯¹åº”çš„ .txt æ–‡ä»¶

#### 4.2.3 å›¾ç‰‡å°ºå¯¸å¤„ç†

```python
def image_resize(img, max_size=512):
    w, h = img.size
    if w >= h:
        new_w = max_size
        new_h = int((max_size / w) * h)
    else:
        new_h = max_size
        new_w = int((max_size / h) * w)
    return img.resize((new_w, new_h))
```

**ä¸ºä»€ä¹ˆè¦è°ƒæ•´å¤§å°ï¼Ÿ**
- ä¸åŒå›¾ç‰‡å¤§å°ä¸ä¸€æ ·
- éœ€è¦ç»Ÿä¸€å¤§å°æ‰èƒ½æ‰¹é‡å¤„ç†
- åŒæ—¶ä¿æŒé•¿å®½æ¯”ï¼Œé¿å…å›¾ç‰‡å˜å½¢

### 4.3 train_configs/train_lora.yaml - é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶å®šä¹‰äº†æ‰€æœ‰è®­ç»ƒå‚æ•°ï¼š

```yaml
# æ¨¡å‹è·¯å¾„
pretrained_model_name_or_path: Qwen/Qwen-Image

# æ•°æ®é…ç½®
data_config:
  img_dir: ./your_lora_dataset    # å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
  img_size: 1024                  # å›¾ç‰‡å¤§å°
  train_batch_size: 1             # æ‰¹æ¬¡å¤§å°
  num_workers: 4                  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
  caption_dropout_rate: 0.1       # æ–‡æœ¬ä¸¢å¼ƒç‡ï¼ˆæ­£åˆ™åŒ–æŠ€å·§ï¼‰

# è®­ç»ƒå‚æ•°
max_train_steps: 3000             # æ€»è®­ç»ƒæ­¥æ•°
learning_rate: 1e-4               # å­¦ä¹ ç‡
train_batch_size: 1               # æ‰¹æ¬¡å¤§å°
gradient_accumulation_steps: 1    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

# LoRA å‚æ•°
rank: 16                          # LoRA ç§©

# è¾“å‡º
output_dir: ./output              # æ¨¡å‹ä¿å­˜è·¯å¾„
checkpointing_steps: 250          # æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡

# ä¼˜åŒ–å™¨å‚æ•°
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01

# æ··åˆç²¾åº¦
mixed_precision: "bf16"           # ä½¿ç”¨ bfloat16 æ··åˆç²¾åº¦
```

**é‡è¦å‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | å»ºè®®å€¼ | è¯´æ˜ |
|------|--------|------|
| `learning_rate` | 1e-4 åˆ° 5e-4 | å­¦ä¹ ç‡ï¼Œå¤ªå¤§ä¸ç¨³å®šï¼Œå¤ªå°å­¦å¾—æ…¢ |
| `rank` | 8-32 | LoRA ç§©ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼º |
| `max_train_steps` | 1000-5000 | æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´ |
| `img_size` | 512-1024 | å–å†³äºæ˜¾å­˜å¤§å° |
| `gradient_accumulation_steps` | 1-8 | ç”¨äºæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡ |

### 4.4 inference.py - æ¨ç†è„šæœ¬

è®­ç»ƒå®Œæˆåï¼Œç”¨è¿™ä¸ªè„šæœ¬ç”Ÿæˆå›¾ç‰‡ï¼š

```python
# 1. åŠ è½½æ¨¡å‹
pipe = DiffusionPipeline.from_pretrained(
    "Qwen/Qwen-Image", 
    torch_dtype=torch.bfloat16
)

# 2. åŠ è½½ LoRA æƒé‡
pipe.load_lora_weights('./output/checkpoint-1000')

# 3. ç”Ÿæˆå›¾ç‰‡
image = pipe(
    prompt="ä½ çš„æç¤ºè¯",
    width=1024,
    height=1024,
    num_inference_steps=50,
)

# 4. ä¿å­˜å›¾ç‰‡
image.images[0].save("output.png")
```

---

## 5. æ•°æ®å‡†å¤‡è¯¦è§£

æ•°æ®å‡†å¤‡æ˜¯è®­ç»ƒæˆåŠŸçš„å…³é”®ï¼

### 5.1 æ•°æ®é›†ç»“æ„

**Qwen-Image å’Œ FLUX çš„æ•°æ®ç»“æ„ï¼š**

```
my_dataset/
â”œâ”€â”€ image1.jpg
â”œâ”€â”€ image1.txt      # image1.jpg çš„æè¿°
â”œâ”€â”€ image2.png
â”œâ”€â”€ image2.txt      # image2.png çš„æè¿°
â”œâ”€â”€ image3.jpg
â”œâ”€â”€ image3.txt
â””â”€â”€ ...
```

**Qwen-Image-Edit çš„æ•°æ®ç»“æ„ï¼š**

```
my_edit_dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ img001.jpg
â”‚   â”œâ”€â”€ img001.txt  # ç›®æ ‡å›¾ç‰‡æè¿°
â”‚   â”œâ”€â”€ img002.jpg
â”‚   â””â”€â”€ img002.txt
â””â”€â”€ control/
    â”œâ”€â”€ img001.jpg  # æ§åˆ¶å›¾ï¼ˆåŸå§‹å›¾ï¼‰
    â”œâ”€â”€ img002.jpg
    â””â”€â”€ ...
```

### 5.2 å‡†å¤‡å›¾ç‰‡

**å›¾ç‰‡è¦æ±‚ï¼š**

1. **æ ¼å¼**ï¼šJPGã€PNGã€WEBP éƒ½å¯ä»¥
2. **åˆ†è¾¨ç‡**ï¼šå»ºè®® 1024Ã—1024 æˆ–æ›´é«˜
3. **è´¨é‡**ï¼šè¶Šé«˜è¶Šå¥½ï¼Œé¿å…æ¨¡ç³Šã€æœ‰ç‘•ç–µçš„å›¾ç‰‡
4. **æ•°é‡**ï¼š
   - æœ€å°‘ï¼š10 å¼ 
   - æ¨èï¼š20-50 å¼ 
   - ç†æƒ³ï¼š100+ å¼ 

**å›¾ç‰‡ç±»å‹å»ºè®®ï¼š**

å¯¹äº FLUX äººåƒè®­ç»ƒï¼š
- åŒä¸€ä¸ªäººçš„ä¸åŒç…§ç‰‡
- ä¸åŒè§’åº¦ã€è¡¨æƒ…ã€å…‰çº¿
- é«˜è´¨é‡çš„æ¸…æ™°ç…§ç‰‡

å¯¹äº Qwen-Image é£æ ¼è®­ç»ƒï¼š
- é£æ ¼ç»Ÿä¸€çš„å›¾ç‰‡é›†åˆ
- å¯ä»¥æ˜¯ç‰¹å®šè‰ºæœ¯é£æ ¼ã€ç‰¹å®šä¸»é¢˜

### 5.3 ç¼–å†™æ–‡æœ¬æè¿°

**æ–‡æœ¬æè¿°ï¼ˆCaptionï¼‰éå¸¸é‡è¦ï¼**

#### 5.3.1 Qwen-Image æè¿°ç¤ºä¾‹

```
# image1.txt
ä¸€ä½å¹´è½»å¥³æ€§çš„ä¸“ä¸šè‚–åƒç…§ï¼Œæ¼”æ’­å®¤ç¯å…‰ï¼Œä¼˜é›…çš„å§¿åŠ¿ï¼Œçœ‹ç€é•œå¤´ï¼ŒæŸ”å’Œçš„é˜´å½±ï¼Œé«˜è´¨é‡ï¼Œè¯¦ç»†çš„é¢éƒ¨ç‰¹å¾ï¼Œç”µå½±èˆ¬çš„ç¯å…‰
```

#### 5.3.2 FLUX æè¿°ç¤ºä¾‹ï¼ˆéœ€è¦è§¦å‘è¯ï¼‰

```
# portrait1.txt
ohwx woman, professional headshot, studio lighting, elegant pose, looking at camera

# portrait2.txt
ohwx woman, casual outdoor photo, natural lighting, smiling, park background

# portrait3.txt
ohwx woman, close-up portrait, dramatic lighting, serious expression
```

**é‡è¦æç¤ºï¼š**
- FLUX è®­ç»ƒéœ€è¦ä½¿ç”¨è§¦å‘è¯ï¼Œå¦‚ `ohwx woman` æˆ– `ohwx man`
- Qwen-Image ä¸éœ€è¦ç‰¹æ®Šè§¦å‘è¯

#### 5.3.3 è‡ªåŠ¨ç”Ÿæˆæè¿°

å¦‚æœä½ ä¸æƒ³æ‰‹å†™æè¿°ï¼Œå¯ä»¥ä½¿ç”¨ AI å·¥å…·è‡ªåŠ¨ç”Ÿæˆï¼š

1. **Florence-2**ï¼šhttps://huggingface.co/spaces/gokaygokay/Florence-2
   - ä¸Šä¼ å›¾ç‰‡ï¼Œè‡ªåŠ¨ç”Ÿæˆè‹±æ–‡æè¿°
   
2. **BLIP-2**ï¼šå¦ä¸€ä¸ªå›¾åƒæè¿°ç”Ÿæˆå·¥å…·

3. **GPT-4V**ï¼šå¦‚æœä½ æœ‰ ChatGPT Plusï¼Œå¯ä»¥ä¸Šä¼ å›¾ç‰‡è®©å®ƒå†™æè¿°

### 5.4 éªŒè¯æ•°æ®é›†

ä½¿ç”¨é¡¹ç›®æä¾›çš„éªŒè¯å·¥å…·ï¼š

```bash
python utils/validate_dataset.py --path ./your_dataset
```

**éªŒè¯å†…å®¹ï¼š**
- âœ… æ¯å¼ å›¾ç‰‡éƒ½æœ‰å¯¹åº”çš„ .txt æ–‡ä»¶
- âœ… æ–‡ä»¶å‘½åæ­£ç¡®
- âœ… æ–‡æœ¬æ–‡ä»¶ä¸ä¸ºç©º
- âš ï¸ å¦‚æœ‰é—®é¢˜ä¼šæç¤ºå…·ä½“æ˜¯å“ªä¸ªæ–‡ä»¶

---

## 6. è®­ç»ƒæµç¨‹è¯¦è§£

### 6.1 ç¯å¢ƒå‡†å¤‡

#### 6.1.1 å®‰è£… Python

ç¡®ä¿ä½ æœ‰ Python 3.10ï¼š

```bash
python --version  # åº”è¯¥æ˜¾ç¤º Python 3.10.x
```

#### 6.1.2 å…‹éš†ä»“åº“

```bash
git clone https://github.com/FlyMyAI/flymyai-lora-trainer
cd flymyai-lora-trainer
```

#### 6.1.3 å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
pip install git+https://github.com/huggingface/diffusers
```

**requirements.txt é‡Œæœ‰ä»€ä¹ˆï¼Ÿ**

```
accelerate==1.9.0       # è®­ç»ƒåŠ é€Ÿ
diffusers               # Hugging Face æ‰©æ•£æ¨¡å‹åº“
transformers            # Transformer æ¨¡å‹åº“
peft==0.17.0           # LoRA å®ç°
torch                   # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
...
```

### 6.2 å‡†å¤‡è®­ç»ƒ

#### 6.2.1 é€‰æ‹©è®­ç»ƒè„šæœ¬

æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©ï¼š

| éœ€æ±‚ | ä½¿ç”¨è„šæœ¬ | æ˜¾å­˜è¦æ±‚ |
|------|----------|----------|
| Qwen-Image LoRA | `train.py` | 40GB+ |
| Qwen-Image LoRA (ä½æ˜¾å­˜) | `train_4090.py` | 24GB |
| FLUX LoRA | `train_flux_lora.py` | 40GB+ |
| Qwen-Image-Edit LoRA | `train_qwen_edit_lora.py` | 40GB+ |

#### 6.2.2 ä¿®æ”¹é…ç½®æ–‡ä»¶

æ‰“å¼€å¯¹åº”çš„ YAML é…ç½®æ–‡ä»¶ï¼Œä¿®æ”¹ï¼š

```yaml
data_config:
  img_dir: ./your_dataset    # æ”¹æˆä½ çš„æ•°æ®é›†è·¯å¾„
  
output_dir: ./my_lora_output  # æ”¹æˆä½ æƒ³è¦çš„è¾“å‡ºè·¯å¾„

max_train_steps: 2000         # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´
```

#### 6.2.3 å¯åŠ¨è®­ç»ƒ

```bash
# Qwen-Image LoRA
accelerate launch train.py --config ./train_configs/train_lora.yaml

# FLUX LoRA
accelerate launch train_flux_lora.py --config ./train_configs/train_flux_config.yaml
```

### 6.3 è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

#### 6.3.1 åˆå§‹åŒ–é˜¶æ®µ

```
[INFO] Loading model from Qwen/Qwen-Image...
[INFO] Loading VAE...
[INFO] Loading transformer...
[INFO] Adding LoRA adapters...
[INFO] Total trainable parameters: 16.7M
```

**è¿™ä¸ªé˜¶æ®µåšä»€ä¹ˆï¼Ÿ**
- ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼‰
- åŠ è½½æ¨¡å‹ç»„ä»¶
- åˆå§‹åŒ– LoRA å±‚
- å‡†å¤‡è®­ç»ƒ

#### 6.3.2 è®­ç»ƒé˜¶æ®µ

```
Steps: 0%|          | 0/2000 [00:00<?, ?it/s]
Steps: 1%|â–         | 10/2000 [00:45<2:30:15, loss=0.1234]
Steps: 2%|â–         | 50/2000 [03:45<2:28:00, loss=0.0856]
```

**è¿›åº¦æ¡æ˜¾ç¤ºï¼š**
- å½“å‰æ­¥æ•° / æ€»æ­¥æ•°
- é¢„è®¡å‰©ä½™æ—¶é—´
- å½“å‰æŸå¤±å€¼

**æŸå¤±å€¼ï¼ˆlossï¼‰çš„å˜åŒ–ï¼š**
- å¼€å§‹æ—¶è¾ƒå¤§ï¼ˆæ¯”å¦‚ 0.5ï¼‰
- é€æ¸ä¸‹é™ï¼ˆæ¯”å¦‚ 0.1ã€0.05ï¼‰
- æœ€ç»ˆç¨³å®šåœ¨ä¸€ä¸ªè¾ƒå°çš„å€¼

**æ³¨æ„ï¼š**
- æŸå¤±å€¼ä¸‹é™æ˜¯å¥½äº‹ï¼Œè¯´æ˜æ¨¡å‹åœ¨å­¦ä¹ 
- å¦‚æœæŸå¤±ä¸å†ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡
- å¦‚æœæŸå¤±çªç„¶å‡é«˜ï¼Œå¯èƒ½å­¦ä¹ ç‡å¤ªå¤§

#### 6.3.3 ä¿å­˜æ£€æŸ¥ç‚¹

æ¯éš” `checkpointing_steps` æ­¥ï¼Œæ¨¡å‹ä¼šä¿å­˜ä¸€æ¬¡ï¼š

```
[INFO] Saved checkpoint to ./output/checkpoint-250
[INFO] Saved checkpoint to ./output/checkpoint-500
[INFO] Saved checkpoint to ./output/checkpoint-750
```

**æ£€æŸ¥ç‚¹åŒ…å«ï¼š**
```
checkpoint-250/
â””â”€â”€ pytorch_lora_weights.safetensors  # LoRA æƒé‡æ–‡ä»¶
```

### 6.4 è®­ç»ƒæ—¶é—´ä¼°ç®—

**Qwen-Image LoRA (3000 æ­¥)ï¼š**
- RTX 4090 (24GB)ï¼šçº¦ 4-6 å°æ—¶
- A100 (40GB)ï¼šçº¦ 2-3 å°æ—¶

**FLUX LoRA (2000 æ­¥)ï¼š**
- RTX 4090 (24GB)ï¼šçº¦ 6-8 å°æ—¶ï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆï¼‰
- A100 (80GB)ï¼šçº¦ 3-4 å°æ—¶

### 6.5 è®­ç»ƒå®Œæˆ

```
[INFO] Training completed!
[INFO] Final checkpoint saved to ./output/checkpoint-3000
```

**è®­ç»ƒå®Œæˆåï¼Œä½ ä¼šå¾—åˆ°ï¼š**
- LoRA æƒé‡æ–‡ä»¶ (`pytorch_lora_weights.safetensors`)
- è®­ç»ƒæ—¥å¿—
- å¯èƒ½è¿˜æœ‰ä¸€äº›ç¤ºä¾‹ç”Ÿæˆå›¾ç‰‡ï¼ˆå¦‚æœé…ç½®äº†é‡‡æ ·ï¼‰

---

## 7. æ¨ç†ä½¿ç”¨è¯¦è§£

è®­ç»ƒå®Œæˆåï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ LoRA ç”Ÿæˆå›¾ç‰‡ï¼

### 7.1 ä½¿ç”¨ Python è„šæœ¬

#### 7.1.1 Qwen-Image æ¨ç†

```python
from diffusers import DiffusionPipeline
import torch

# åŠ è½½åŸºç¡€æ¨¡å‹
pipe = DiffusionPipeline.from_pretrained(
    "Qwen/Qwen-Image",
    torch_dtype=torch.bfloat16
)
pipe.to("cuda")

# åŠ è½½ä½ è®­ç»ƒçš„ LoRA
pipe.load_lora_weights('./output/checkpoint-3000')

# ç”Ÿæˆå›¾ç‰‡
prompt = "ä¸€ä½å¹´è½»å¥³æ€§çš„ä¸“ä¸šè‚–åƒç…§ï¼Œæ¼”æ’­å®¤ç¯å…‰ï¼Œä¼˜é›…çš„å§¿åŠ¿"
image = pipe(
    prompt=prompt,
    width=1024,
    height=1024,
    num_inference_steps=50,
    true_cfg_scale=5,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

# ä¿å­˜
image.save("output.png")
```

#### 7.1.2 FLUX æ¨ç†

```python
from diffusers import DiffusionPipeline
import torch

# åŠ è½½ FLUX æ¨¡å‹
pipe = DiffusionPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.bfloat16
)
pipe.to("cuda")

# åŠ è½½ LoRA
pipe.load_lora_weights('./flux_output/checkpoint-2000')

# ç”Ÿæˆå›¾ç‰‡ï¼ˆæ³¨æ„ä½¿ç”¨è§¦å‘è¯ï¼‰
prompt = "ohwx woman, professional headshot, studio lighting"
image = pipe(
    prompt=prompt,
    width=1024,
    height=1024,
    num_inference_steps=30,
    guidance_scale=3.5,
).images[0]

image.save("flux_output.png")
```

### 7.2 ä½¿ç”¨é¡¹ç›®çš„æ¨ç†è„šæœ¬

```bash
python inference.py \
  --model_name Qwen/Qwen-Image \
  --lora_weights ./output/checkpoint-3000 \
  --prompt "ä½ çš„æç¤ºè¯" \
  --output_image output.png \
  --width 1024 \
  --height 1024 \
  --num_inference_steps 50
```

### 7.3 é‡è¦å‚æ•°è¯´æ˜

#### 7.3.1 num_inference_stepsï¼ˆæ¨ç†æ­¥æ•°ï¼‰

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- æ¨¡å‹å»å™ªçš„æ­¥æ•°
- æ­¥æ•°è¶Šå¤šï¼Œå›¾ç‰‡è´¨é‡è¶Šå¥½ï¼Œä½†ç”Ÿæˆè¶Šæ…¢

**å»ºè®®å€¼ï¼š**
- å¿«é€Ÿæµ‹è¯•ï¼š20-30 æ­¥
- æ­£å¸¸ä½¿ç”¨ï¼š30-50 æ­¥
- é«˜è´¨é‡ï¼š50-100 æ­¥

#### 7.3.2 CFG Scale / Guidance Scaleï¼ˆå¼•å¯¼å¼ºåº¦ï¼‰

**è¿™æ˜¯ä»€ä¹ˆï¼Ÿ**
- æ§åˆ¶æ¨¡å‹éµå¾ªæç¤ºè¯çš„ç¨‹åº¦
- å€¼è¶Šå¤§ï¼Œè¶Šä¸¥æ ¼æŒ‰ç…§æç¤ºè¯ç”Ÿæˆ
- å€¼è¶Šå°ï¼Œæ¨¡å‹è¶Šè‡ªç”±å‘æŒ¥

**å»ºè®®å€¼ï¼š**
- Qwen-Imageï¼š3-7ï¼ˆ`true_cfg_scale`ï¼‰
- FLUXï¼š2-5ï¼ˆ`guidance_scale`ï¼‰

#### 7.3.3 Width å’Œ Heightï¼ˆå›¾ç‰‡å°ºå¯¸ï¼‰

**å»ºè®®å€¼ï¼š**
- å¸¸ç”¨ï¼š1024Ã—1024ï¼ˆæ­£æ–¹å½¢ï¼‰
- æ¨ªç‰ˆï¼š1280Ã—720 æˆ– 1920Ã—1080
- ç«–ç‰ˆï¼š720Ã—1280

**æ³¨æ„ï¼š**
- å°ºå¯¸å¿…é¡»æ˜¯ 8 çš„å€æ•°ï¼ˆQwenï¼‰æˆ– 64 çš„å€æ•°ï¼ˆFLUXï¼‰
- å°ºå¯¸è¶Šå¤§ï¼Œéœ€è¦è¶Šå¤šæ˜¾å­˜

#### 7.3.4 Seedï¼ˆéšæœºç§å­ï¼‰

```python
generator = torch.Generator(device="cuda").manual_seed(42)
```

**ä½œç”¨ï¼š**
- å›ºå®šéšæœºç§å­å¯ä»¥å¾—åˆ°**å¯å¤ç°çš„ç»“æœ**
- åŒæ ·çš„ç§å­ + åŒæ ·çš„æç¤ºè¯ = åŒæ ·çš„å›¾ç‰‡
- ç”¨äºè°ƒè¯•å’Œæ¯”è¾ƒ

### 7.4 ä½¿ç”¨ ComfyUIï¼ˆå›¾å½¢ç•Œé¢ï¼‰

å¦‚æœä½ ä¸å–œæ¬¢å†™ä»£ç ï¼Œå¯ä»¥ä½¿ç”¨ ComfyUIï¼š

1. å®‰è£… ComfyUI
2. ä¸‹è½½ Qwen-Image æ¨¡å‹æ–‡ä»¶
3. å°†ä½ çš„ LoRA æ–‡ä»¶æ”¾åˆ° `ComfyUI/models/loras/`
4. å¯¼å…¥é¡¹ç›®æä¾›çš„å·¥ä½œæµæ–‡ä»¶ `qwen_image_lora_example.json`
5. åœ¨ç•Œé¢ä¸Šé€‰æ‹©ä½ çš„ LoRA
6. è¾“å…¥æç¤ºè¯ï¼Œç”Ÿæˆå›¾ç‰‡

---

## 8. å®æˆ˜ç¤ºä¾‹

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼Œä»å¤´åˆ°å°¾èµ°ä¸€éæµç¨‹ã€‚

### 8.1 åœºæ™¯ï¼šè®­ç»ƒä¸€ä¸ªç‰¹å®šäººç‰©çš„ FLUX LoRA

**ç›®æ ‡ï¼š** è®­ç»ƒä¸€ä¸ªèƒ½ç”Ÿæˆç‰¹å®šäººç‰©ï¼ˆæ¯”å¦‚ä½ è‡ªå·±æˆ–ä½ çš„æœ‹å‹ï¼‰ç…§ç‰‡çš„ LoRA

#### æ­¥éª¤ 1ï¼šå‡†å¤‡æ•°æ®

æ”¶é›† 20-30 å¼ è¿™ä¸ªäººçš„ç…§ç‰‡ï¼š
- ä¸åŒè§’åº¦
- ä¸åŒè¡¨æƒ…
- ä¸åŒå…‰çº¿
- ä¸åŒèƒŒæ™¯

```
my_person_dataset/
â”œâ”€â”€ photo01.jpg
â”œâ”€â”€ photo02.jpg
â”œâ”€â”€ photo03.jpg
...
â”œâ”€â”€ photo20.jpg
```

#### æ­¥éª¤ 2ï¼šç”Ÿæˆæ–‡æœ¬æè¿°

ä¸ºæ¯å¼ å›¾ç‰‡å†™æè¿°ï¼ˆæˆ–ä½¿ç”¨ Florence-2 è‡ªåŠ¨ç”Ÿæˆï¼‰ï¼š

```
# photo01.txt
ohwx woman, professional headshot, studio lighting, neutral expression

# photo02.txt
ohwx woman, outdoor photo, natural sunlight, smiling, park background

# photo03.txt
ohwx woman, close-up portrait, soft lighting, looking at camera
```

**å…³é”®ç‚¹ï¼š**
- æ¯ä¸ªæè¿°éƒ½ä»¥ `ohwx woman` å¼€å¤´ï¼ˆè§¦å‘è¯ï¼‰
- æè¿°è¦å‡†ç¡®åæ˜ å›¾ç‰‡å†…å®¹

#### æ­¥éª¤ 3ï¼šéªŒè¯æ•°æ®é›†

```bash
python utils/validate_dataset.py --path ./my_person_dataset
```

è¾“å‡ºï¼š
```
âœ… Found 20 images
âœ… All images have corresponding text files
âœ… No issues found
```

#### æ­¥éª¤ 4ï¼šä¿®æ”¹é…ç½®

ç¼–è¾‘ `train_configs/train_flux_config.yaml`ï¼š

```yaml
data_config:
  img_dir: ./my_person_dataset  # ä½ çš„æ•°æ®é›†è·¯å¾„
  img_size: 1024
  train_batch_size: 1
  
output_dir: ./my_person_lora
max_train_steps: 1500            # 20 å¼ å›¾ï¼Œ1500 æ­¥è¶³å¤Ÿ
learning_rate: 4e-4
rank: 16
```

#### æ­¥éª¤ 5ï¼šå¼€å§‹è®­ç»ƒ

```bash
accelerate launch train_flux_lora.py --config ./train_configs/train_flux_config.yaml
```

ç­‰å¾… 6-8 å°æ—¶ï¼ˆRTX 4090ï¼‰...

#### æ­¥éª¤ 6ï¼šæµ‹è¯• LoRA

```python
from diffusers import DiffusionPipeline
import torch

pipe = DiffusionPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.bfloat16
)
pipe.to("cuda")
pipe.load_lora_weights('./my_person_lora/checkpoint-1500')

# æµ‹è¯•ä¸åŒçš„åœºæ™¯
prompts = [
    "ohwx woman, professional business photo, wearing suit, office background",
    "ohwx woman, casual beach photo, summer vibes, sunset lighting",
    "ohwx woman, artistic portrait, dramatic lighting, black and white",
]

for i, prompt in enumerate(prompts):
    image = pipe(prompt=prompt, num_inference_steps=30).images[0]
    image.save(f"test_{i}.png")
```

#### æ­¥éª¤ 7ï¼šè¯„ä¼°ç»“æœ

æŸ¥çœ‹ç”Ÿæˆçš„å›¾ç‰‡ï¼š
- âœ… äººç‰©ç‰¹å¾æ˜¯å¦å‡†ç¡®ï¼Ÿ
- âœ… å›¾ç‰‡è´¨é‡æ˜¯å¦é«˜ï¼Ÿ
- âœ… æ˜¯å¦èƒ½åœ¨ä¸åŒåœºæ™¯ä¸‹ä¿æŒä¸€è‡´ï¼Ÿ

**å¦‚æœæ•ˆæœä¸å¥½ï¼š**
- å°è¯•å¢åŠ è®­ç»ƒæ­¥æ•°
- æ£€æŸ¥æ•°æ®é›†è´¨é‡
- è°ƒæ•´å­¦ä¹ ç‡
- å¢åŠ æ›´å¤šè®­ç»ƒå›¾ç‰‡

---

## 9. å¸¸è§é—®é¢˜

### 9.1 è®­ç»ƒç›¸å…³

#### Q1: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. ä½¿ç”¨ä½æ˜¾å­˜ç‰ˆæœ¬è®­ç»ƒè„šæœ¬ï¼š
   ```bash
   accelerate launch train_4090.py --config ./train_configs/train_lora_4090.yaml
   ```

2. å‡å°æ‰¹æ¬¡å¤§å°ï¼š
   ```yaml
   train_batch_size: 1  # æ”¹ä¸º 1
   ```

3. å‡å°å›¾ç‰‡å¤§å°ï¼š
   ```yaml
   img_size: 512  # ä» 1024 æ”¹ä¸º 512
   ```

4. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š
   ```yaml
   gradient_accumulation_steps: 4  # æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡
   ```

#### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. ä½¿ç”¨æ··åˆç²¾åº¦ï¼š
   ```yaml
   mixed_precision: "bf16"  # æˆ– "fp16"
   ```

2. å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹ï¼š
   ```yaml
   num_workers: 8  # å¢åŠ åˆ° 8
   ```

3. ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨ï¼ˆSSD è€Œä¸æ˜¯ HDDï¼‰

4. å‡å°‘è®­ç»ƒæ­¥æ•°ï¼ˆä½†å¯èƒ½å½±å“æ•ˆæœï¼‰

#### Q3: æŸå¤±ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š**

1. **å­¦ä¹ ç‡å¤ªå°**ï¼š
   ```yaml
   learning_rate: 5e-4  # ä» 1e-4 å¢åŠ åˆ° 5e-4
   ```

2. **æ•°æ®é—®é¢˜**ï¼š
   - æ£€æŸ¥å›¾ç‰‡å’Œæ–‡æœ¬æ˜¯å¦å¯¹åº”
   - ç¡®ä¿æ–‡æœ¬æè¿°å‡†ç¡®

3. **è®­ç»ƒæ­¥æ•°å¤ªå°‘**ï¼š
   - å¢åŠ  `max_train_steps`

#### Q4: è¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶ï¼š**
- è®­ç»ƒæŸå¤±å¾ˆä½ï¼Œä½†ç”Ÿæˆæ•ˆæœä¸å¥½
- åªèƒ½ç”Ÿæˆå’Œè®­ç»ƒé›†å¾ˆåƒçš„å›¾ç‰‡

**è§£å†³æ–¹æ¡ˆï¼š**

1. å¢åŠ æ•°æ®é›†ï¼š
   - è‡³å°‘ 30-50 å¼ å›¾ç‰‡

2. ä½¿ç”¨ caption dropoutï¼š
   ```yaml
   caption_dropout_rate: 0.1  # 10% çš„æ¦‚ç‡ä¸¢å¼ƒæ–‡æœ¬
   ```

3. å‡å°‘è®­ç»ƒæ­¥æ•°

4. é™ä½ LoRA rankï¼š
   ```yaml
   rank: 8  # ä» 16 é™åˆ° 8
   ```

### 9.2 æ¨ç†ç›¸å…³

#### Q1: ç”Ÿæˆçš„å›¾ç‰‡ä¸åƒè®­ç»ƒçš„å†…å®¹ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. **æ£€æŸ¥è§¦å‘è¯**ï¼ˆFLUX å¿…é¡»ï¼‰ï¼š
   ```python
   prompt = "ohwx woman, ..."  # ä¸€å®šè¦åŠ è§¦å‘è¯
   ```

2. **å¢åŠ æ¨ç†æ­¥æ•°**ï¼š
   ```python
   num_inference_steps=50  # ä» 20 å¢åŠ åˆ° 50
   ```

3. **è°ƒæ•´ CFG/Guidance Scale**ï¼š
   ```python
   true_cfg_scale=6  # Qwenï¼Œå°è¯• 5-7
   guidance_scale=4  # FLUXï¼Œå°è¯• 3-5
   ```

#### Q2: ç”Ÿæˆçš„å›¾ç‰‡è´¨é‡ä¸å¥½ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. å¢åŠ æ¨ç†æ­¥æ•°ï¼ˆ50-100 æ­¥ï¼‰
2. ä½¿ç”¨æ›´å¥½çš„æç¤ºè¯
3. å°è¯•ä¸åŒçš„éšæœºç§å­
4. æ£€æŸ¥ LoRA æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½

#### Q3: ç”Ÿæˆé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. ä½¿ç”¨é‡åŒ–æ¨ç†ï¼š
   ```python
   # ä½¿ç”¨ inference.pyï¼Œå®ƒåŒ…å«é‡åŒ–ä¼˜åŒ–
   ```

2. å‡å°‘æ¨ç†æ­¥æ•°ï¼ˆä½†ä¼šå½±å“è´¨é‡ï¼‰

3. ä½¿ç”¨æ›´å°çš„å›¾ç‰‡å°ºå¯¸

4. ä½¿ç”¨ CPU offloadï¼š
   ```python
   pipe.enable_model_cpu_offload()
   ```

### 9.3 ç¯å¢ƒç›¸å…³

#### Q1: å®‰è£…ä¾èµ–å¤±è´¥ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. ç¡®ä¿ Python ç‰ˆæœ¬æ­£ç¡®ï¼ˆ3.10ï¼‰
2. æ›´æ–° pipï¼š
   ```bash
   pip install --upgrade pip
   ```
3. å•ç‹¬å®‰è£…å¤±è´¥çš„åŒ…
4. ä½¿ç”¨ conda ç¯å¢ƒ

#### Q2: CUDA out of memory é”™è¯¯ï¼Ÿ

è¿™æ˜¯æ˜¾å­˜ä¸è¶³ï¼Œå‚è§ Q9.1.1 çš„è§£å†³æ–¹æ¡ˆã€‚

---

## ğŸ“ æ€»ç»“

æ­å–œä½ è¯»åˆ°è¿™é‡Œï¼ç°åœ¨ä½ åº”è¯¥å¯¹è¿™ä¸ªé¡¹ç›®æœ‰äº†å…¨é¢çš„ç†è§£ã€‚

### ä½ å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ

âœ… LoRA æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨¡å‹å¾®è°ƒæŠ€æœ¯
âœ… æ‰©æ•£æ¨¡å‹çš„å·¥ä½œåŸç†
âœ… é¡¹ç›®çš„ç›®å½•ç»“æ„å’Œæ–‡ä»¶ä½œç”¨
âœ… è®­ç»ƒæ•°æ®çš„å‡†å¤‡æ–¹æ³•
âœ… è®­ç»ƒå’Œæ¨ç†çš„å®Œæ•´æµç¨‹
âœ… å¸¸è§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ

### ä¸‹ä¸€æ­¥åšä»€ä¹ˆï¼Ÿ

1. **åŠ¨æ‰‹å®è·µ**ï¼šå‡†å¤‡ä¸€ä¸ªå°æ•°æ®é›†ï¼Œå°è¯•è®­ç»ƒç¬¬ä¸€ä¸ª LoRA
2. **å®éªŒå‚æ•°**ï¼šå°è¯•ä¸åŒçš„å­¦ä¹ ç‡ã€rank ç­‰å‚æ•°
3. **ä¼˜åŒ–æç¤ºè¯**ï¼šå­¦ä¹ å¦‚ä½•å†™å‡ºæ›´å¥½çš„ prompt
4. **åˆ†äº«æˆæœ**ï¼šå°†ä½ è®­ç»ƒçš„ LoRA åˆ†äº«åˆ° Hugging Face

### å­¦ä¹ èµ„æº

- **Hugging Face Diffusers æ–‡æ¡£**ï¼šhttps://huggingface.co/docs/diffusers
- **PEFT (LoRA) æ–‡æ¡£**ï¼šhttps://huggingface.co/docs/peft
- **FlyMy.AI æ–‡æ¡£**ï¼šhttps://docs.flymy.ai
- **Discord ç¤¾åŒº**ï¼šhttps://discord.com/invite/t6hPBpSebw

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœä½ åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. **æŸ¥çœ‹æœ¬æ•™ç¨‹**ï¼šä»”ç»†é˜…è¯»ç›¸å…³ç« èŠ‚
2. **æŸ¥çœ‹å¸¸è§é—®é¢˜**ï¼šç¬¬ 9 ç« å¯èƒ½å·²ç»å›ç­”äº†ä½ çš„é—®é¢˜
3. **æŸ¥çœ‹ GitHub Issues**ï¼šçœ‹çœ‹åˆ«äººæ˜¯å¦é‡åˆ°è¿‡ç±»ä¼¼é—®é¢˜
4. **åŠ å…¥ Discord**ï¼šåœ¨ç¤¾åŒºå¯»æ±‚å¸®åŠ©
5. **æäº¤ Issue**ï¼šå¦‚æœå‘ç° bugï¼Œåœ¨ GitHub æäº¤ issue

---

**ç¥ä½ è®­ç»ƒæ„‰å¿«ï¼ğŸš€**
